{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9780d62e-2980-41d5-bbd1-b3871752fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49660713-bc3a-4d41-89df-7bd10e263b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53bfe8c6-2d44-4a6a-be5b-abfa022b6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_path = 'data/Fake.csv'\n",
    "true_path = 'data/True.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "423305e3-f890-4892-821e-b88660325296",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = pd.read_csv(fake_path)\n",
    "true_data = pd.read_csv(true_path)\n",
    "\n",
    "fake_data['label'] = 'fake'\n",
    "true_data['label'] = 'true'\n",
    "\n",
    "data = pd.concat([fake_data, true_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4652288-3259-419a-98e0-59440da50ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
       "      <td>BRUSSELS (Reuters) - NATO allies on Tuesday we...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "      <td>LONDON (Reuters) - LexisNexis, a provider of l...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>Minsk cultural hub becomes haven from authorities</td>\n",
       "      <td>MINSK (Reuters) - In the shadow of disused Sov...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "      <td>MOSCOW (Reuters) - Vatican Secretary of State ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>Indonesia to buy $1.14 billion worth of Russia...</td>\n",
       "      <td>JAKARTA (Reuters) - Indonesia will buy 11 Sukh...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0       Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1       Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3       Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4       Pope Francis Just Called Out Donald Trump Dur...   \n",
       "...                                                  ...   \n",
       "44893  'Fully committed' NATO backs new U.S. approach...   \n",
       "44894  LexisNexis withdrew two products from Chinese ...   \n",
       "44895  Minsk cultural hub becomes haven from authorities   \n",
       "44896  Vatican upbeat on possibility of Pope Francis ...   \n",
       "44897  Indonesia to buy $1.14 billion worth of Russia...   \n",
       "\n",
       "                                                    text    subject  \\\n",
       "0      Donald Trump just couldn t wish all Americans ...       News   \n",
       "1      House Intelligence Committee Chairman Devin Nu...       News   \n",
       "2      On Friday, it was revealed that former Milwauk...       News   \n",
       "3      On Christmas day, Donald Trump announced that ...       News   \n",
       "4      Pope Francis used his annual Christmas Day mes...       News   \n",
       "...                                                  ...        ...   \n",
       "44893  BRUSSELS (Reuters) - NATO allies on Tuesday we...  worldnews   \n",
       "44894  LONDON (Reuters) - LexisNexis, a provider of l...  worldnews   \n",
       "44895  MINSK (Reuters) - In the shadow of disused Sov...  worldnews   \n",
       "44896  MOSCOW (Reuters) - Vatican Secretary of State ...  worldnews   \n",
       "44897  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...  worldnews   \n",
       "\n",
       "                    date label  \n",
       "0      December 31, 2017  fake  \n",
       "1      December 31, 2017  fake  \n",
       "2      December 30, 2017  fake  \n",
       "3      December 29, 2017  fake  \n",
       "4      December 25, 2017  fake  \n",
       "...                  ...   ...  \n",
       "44893   August 22, 2017   true  \n",
       "44894   August 22, 2017   true  \n",
       "44895   August 22, 2017   true  \n",
       "44896   August 22, 2017   true  \n",
       "44897   August 22, 2017   true  \n",
       "\n",
       "[44898 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecb02304-5eae-44d1-b85e-a650d8fccba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple avant nettoyage :\n",
      "Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.\n",
      "\n",
      "Exemple après nettoyage :\n",
      "donald trump wish american happy new year leave instead give shout enemy hater dishonest fake news medium former reality show star one job country rapidly grows stronger smarter want wish friend supporter enemy hater even dishonest fake news medium happy healthy new year president angry pant tweeted great year america country rapidly grows stronger smarter want wish friend supporter enemy hater even dishonest fake news medium happy healthy new year great year america donald j trump realdonaldtrump december trump tweet went welll expectwhat kind president sends new year greeting like despicable petty infantile gibberish trump lack decency even allow rise gutter long enough wish american citizen happy new year bishop talbert swan talbertswan december one like calvin calvinstowell december impeachment would make great year america also accept regaining control congress miranda yaver mirandayaver december hear talk include many people hate wonder hate alan sandoval alansandoval december us word hater new year wish marlene marlene december say happy new year koren pollitt korencarpenter december trump new year eve tweet happy new year including many enemy fought lost badly know love donald j trump realdonaldtrump december nothing new trump yearstrump directed message enemy hater new year easter thanksgiving anniversary pictwittercomfpaekypa daniel dale ddale december trump holiday tweet clearly presidentialhow long work hallmark becoming president steven goodine sgoodine december always like difference last year filter breaking roy schulze thbthttt december apart teenager us term hater wendy wendywhistles december fucking year old know rainyday december people voted hole thinking would change got power wrong yearold men change year olderphoto andrew burtongetty image\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Télécharger les ressources nécessaires de NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialiser les stopwords et le lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    # Supprimer les caractères spéciaux et les chiffres\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Supprimer les stopwords et appliquer la lemmatisation\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Appliquer le prétraitement sur les colonnes 'title' et 'text'\n",
    "data['cleaned_title'] = data['title'].apply(preprocess_text)\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Afficher un exemple avant et après le nettoyage\n",
    "print(\"Exemple avant nettoyage :\")\n",
    "print(data['text'].iloc[0])\n",
    "print(\"\\nExemple après nettoyage :\")\n",
    "print(data['cleaned_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38b1a49b-6917-4dda-b204-ea78fcc0e3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Bag of Words matrix: (44898, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialiser le CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_features=5000)  # Limiter à 5000 mots les plus fréquents\n",
    "\n",
    "# Appliquer sur les textes nettoyés\n",
    "bow_features = bow_vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "# Afficher la forme de la matrice BoW\n",
    "print(\"Shape of Bag of Words matrix:\", bow_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5bbb557e-0385-4b93-824b-21adda4c0740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0073\n",
      "Epoch [2/10], Loss: 0.0010\n",
      "Epoch [3/10], Loss: 0.0002\n",
      "Epoch [4/10], Loss: 0.0000\n",
      "Epoch [5/10], Loss: 0.0001\n",
      "Epoch [6/10], Loss: 0.0000\n",
      "Epoch [7/10], Loss: 0.0000\n",
      "Epoch [8/10], Loss: 0.0000\n",
      "Epoch [9/10], Loss: 0.0000\n",
      "Epoch [10/10], Loss: 0.0000\n",
      "Accuracy: 99.61%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Vectorisation BoW\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(data['cleaned_text']).toarray()\n",
    "y = (data['label'] == 'true').astype(int).values  # 1 pour true, 0 pour fake\n",
    "\n",
    "# 3. Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Dataset PyTorch\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = FakeNewsDataset(X_train, y_train)\n",
    "test_dataset = FakeNewsDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# 5. Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 6. Modèle LSTM (input = BoW)\n",
    "class LSTMBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape x for LSTM: (batch, seq_len=1, input_size)\n",
    "        x = x.unsqueeze(1)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# 7. Initialisation\n",
    "model = LSTMBinaryClassifier(input_size=5000, hidden_size=128, num_layers=1).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 8. Entraînement\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 9. Évaluation simple\n",
    "model.eval()\n",
    "correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62c0628c-e0e5-440d-84ca-8499447c206a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0117\n",
      "Epoch [2/10], Loss: 0.0017\n",
      "Epoch [3/10], Loss: 0.0048\n",
      "Epoch [4/10], Loss: 0.0003\n",
      "Epoch [5/10], Loss: 0.0002\n",
      "Epoch [6/10], Loss: 0.0038\n",
      "Epoch [7/10], Loss: 0.0000\n",
      "Epoch [8/10], Loss: 0.0000\n",
      "Epoch [9/10], Loss: 0.0000\n",
      "Epoch [10/10], Loss: 0.0000\n",
      "Accuracy: 99.31%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limiter à 5000 mots les plus fréquents\n",
    "X = tfidf_vectorizer.fit_transform(data['cleaned_text']).toarray()\n",
    "y = (data['label'] == 'true').astype(int).values  # 1 pour true, 0 pour fake\n",
    "\n",
    "# 3. Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Dataset PyTorch\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = FakeNewsDataset(X_train, y_train)\n",
    "test_dataset = FakeNewsDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# 5. Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 6. Modèle LSTM (input = BoW)\n",
    "class LSTMBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape x for LSTM: (batch, seq_len=1, input_size)\n",
    "        x = x.unsqueeze(1)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# 7. Initialisation\n",
    "model = LSTMBinaryClassifier(input_size=5000, hidden_size=128, num_layers=1).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 8. Entraînement\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 9. Évaluation simple\n",
    "model.eval()\n",
    "correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02683dfa-8e05-432a-b785-781fa085c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class LSTMBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch_size, seq_len=1, input_size)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "class LSTMExperiment:\n",
    "    def __init__(self, vectorizer, input_size, device):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.device = device\n",
    "        self.model = LSTMBinaryClassifier(input_size=input_size).to(device)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        X = self.vectorizer.fit_transform(data['cleaned_text']).toarray()\n",
    "        y = (data['label'] == 'true').astype(int).values\n",
    "        return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    def run(self, data, num_epochs=10):\n",
    "        X_train, X_test, y_train, y_test = self.prepare_data(data)\n",
    "\n",
    "        train_loader = DataLoader(FakeNewsDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(FakeNewsDataset(X_test, y_test), batch_size=32)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
    "                outputs = self.model(X_batch)\n",
    "                loss = self.criterion(outputs, y_batch)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        # Évaluation\n",
    "        self.model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                outputs = self.model(X_batch)\n",
    "                preds = (outputs >= 0.5).float().cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(y_batch.numpy())\n",
    "\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d62147b1-259a-415d-a078-ee2309521a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Accuracy: 99.33%\n",
      "BoW Accuracy: 99.60%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TF-IDF\n",
    "exp_tfidf = LSTMExperiment(TfidfVectorizer(max_features=5000), input_size=5000, device=device)\n",
    "acc_tfidf = exp_tfidf.run(data)\n",
    "print(f\"TF-IDF Accuracy: {acc_tfidf:.2%}\")\n",
    "\n",
    "# BoW\n",
    "exp_bow = LSTMExperiment(CountVectorizer(max_features=5000), input_size=5000, device=device)\n",
    "acc_bow = exp_bow.run(data)\n",
    "print(f\"BoW Accuracy: {acc_bow:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361f3c7-f171-4531-8dff-b77468f0364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# Dataset\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Modèle LSTM\n",
    "class LSTMBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Classe générique pour BoW / TF-IDF\n",
    "class LSTMExperimentBase:\n",
    "    def __init__(self, input_size, device):\n",
    "        self.device = device\n",
    "        self.model = LSTMBinaryClassifier(input_size=input_size).to(device)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def run(self, X_train, X_test, y_train, y_test, batch_size=32, num_epochs=10):\n",
    "        train_loader = DataLoader(FakeNewsDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(FakeNewsDataset(X_test, y_test), batch_size=batch_size)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
    "                outputs = self.model(X_batch)\n",
    "                loss = self.criterion(outputs, y_batch)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        self.model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                outputs = self.model(X_batch)\n",
    "                preds = (outputs >= 0.5).float().cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(y_batch.numpy())\n",
    "\n",
    "        return accuracy_score(all_labels, all_preds)\n",
    "\n",
    "# Word2Vec\n",
    "class LSTMWord2VecExperiment(LSTMExperimentBase):\n",
    "    def __init__(self, data, device, vector_size=100, max_len=50):\n",
    "        self.max_len = max_len\n",
    "        self.vector_size = vector_size\n",
    "        self.device = device\n",
    "        self.model = LSTMBinaryClassifier(input_size=vector_size).to(device)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        tokenized = [text.lower().split() for text in data['cleaned_text']]\n",
    "        self.w2v_model = Word2Vec(sentences=tokenized, vector_size=vector_size, window=5, min_count=1, workers=4)\n",
    "        self.word_vectors = self.w2v_model.wv\n",
    "\n",
    "    def encode(self, texts):\n",
    "        encoded = []\n",
    "        for sent in texts:\n",
    "            tokens = sent.lower().split()\n",
    "            vecs = [self.word_vectors[w] if w in self.word_vectors else np.zeros(self.vector_size) for w in tokens]\n",
    "            vecs = vecs[:self.max_len] + [np.zeros(self.vector_size)] * (self.max_len - len(vecs)) if len(vecs) < self.max_len else vecs[:self.max_len]\n",
    "            encoded.append(vecs)\n",
    "        return np.array(encoded)\n",
    "\n",
    "    def run_on_data(self, data):\n",
    "        y = (data['label'] == 'true').astype(int).values\n",
    "        X_train_raw, X_test_raw, y_train, y_test = train_test_split(data['cleaned_text'], y, test_size=0.2, random_state=42)\n",
    "        X_train = self.encode(X_train_raw)\n",
    "        X_test = self.encode(X_test_raw)\n",
    "        return super().run(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# BERT\n",
    "class LSTMBERTExperiment(LSTMExperimentBase):\n",
    "    def __init__(self, device, max_len=50):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        self.bert_model.eval()\n",
    "        self.device = device\n",
    "        self.max_len = max_len\n",
    "        super().__init__(input_size=768, device=device)\n",
    "\n",
    "    def encode(self, texts):\n",
    "        embeddings = []\n",
    "        for text in tqdm.tqdm(texts, desc=\"BERT Embeddings\"):\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=self.max_len)\n",
    "            input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert_model(input_ids, attention_mask=attention_mask)\n",
    "                last_hidden_state = outputs.last_hidden_state.squeeze(0)  # (seq_len, 768)\n",
    "            embeddings.append(last_hidden_state.cpu().numpy())\n",
    "        return np.stack(embeddings)\n",
    "\n",
    "    def run_on_data(self, data):\n",
    "        y = (data['label'] == 'true').astype(int).values\n",
    "        X_train_raw, X_test_raw, y_train, y_test = train_test_split(data['cleaned_text'], y, test_size=0.2, random_state=42)\n",
    "        X_train = self.encode(X_train_raw)\n",
    "        X_test = self.encode(X_test_raw)\n",
    "        return super().run(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d5c60-d7be-4566-9071-483acd8c12b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# 1. Détection GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. BoW\n",
    "bow_vectorizer = CountVectorizer(max_features=5000)\n",
    "X_bow = bow_vectorizer.fit_transform(data['cleaned_text']).toarray()\n",
    "y = (data['label'] == 'true').astype(int).values\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "exp_bow = LSTMExperimentBase(input_size=5000, device=device)\n",
    "accuracy_bow = exp_bow.run(X_train_bow, X_test_bow, y_train_bow, y_test_bow)\n",
    "print(f\"BoW Accuracy: {accuracy_bow:.2%}\")\n",
    "\n",
    "# 3. TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['cleaned_text']).toarray()\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "exp_tfidf = LSTMExperimentBase(input_size=5000, device=device)\n",
    "accuracy_tfidf = exp_tfidf.run(X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf)\n",
    "print(f\"TF-IDF Accuracy: {accuracy_tfidf:.2%}\")\n",
    "\n",
    "# 4. Word2Vec\n",
    "exp_w2v = LSTMWord2VecExperiment(data=data, device=device, vector_size=100, max_len=50)\n",
    "accuracy_w2v = exp_w2v.run_on_data(data)\n",
    "print(f\"Word2Vec Accuracy: {accuracy_w2v:.2%}\")\n",
    "\n",
    "# 5. BERT\n",
    "exp_bert = LSTMBERTExperiment(device=device, max_len=50)\n",
    "accuracy_bert = exp_bert.run_on_data(data)\n",
    "print(f\"BERT Accuracy: {accuracy_bert:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
